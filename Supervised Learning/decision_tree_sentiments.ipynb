{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqEWvDIIi9ZyOA8tNA3enB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samsung-ai-course/6-7-edition/blob/main/Supervised%20Learning/decision_tree_sentiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igKmvWRg5OkH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the Dataset\n",
        "data_path = 'https://raw.githubusercontent.com/samsung-ai-course/6-7-edition/refs/heads/main/Supervised%20Learning/Datasets/financial_sentiments.csv'\n",
        "try:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {data_path} was not found.\")\n",
        "    raise\n",
        "\n",
        "# Dataset Overview\n",
        "print(\"\\n### Dataset Overview ###\")\n",
        "print(df.head())\n",
        "print(\"\\n### Dataset Info ###\")\n",
        "print(df.info())\n",
        "print(\"\\n### Missing Values ###\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the data"
      ],
      "metadata": {
        "id": "6DZx6uf-693s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "print(\"\\n### Sentiment Distribution ###\")\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(data=df, x='Sentiment', palette='viridis')\n",
        "plt.title('Sentiment Distribution')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VxbQtuzW6C1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n### Sentence Length Analysis ###\")\n",
        "df['sentence_length'] = df['Sentence'].apply(lambda x: len(x.split()))\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.histplot(df['sentence_length'], kde=True, bins=30, color='blue')\n",
        "plt.title('Distribution of Sentence Lengths')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GGzVeEIU6D9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering - Bag of Words\n",
        "vectorizer_bow = CountVectorizer(max_features=800)\n",
        "X_bow = vectorizer_bow.fit_transform(df['Sentence']).toarray()\n",
        "\n",
        "# Feature Engineering - TF-IDF\n",
        "vectorizer_tfidf = #TODO\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(df['Sentence']).toarray()\n",
        "\n",
        "# Bonus: use sentence transformers to create sementic embedings..\n",
        "#X_semantic =\n",
        "\n",
        "\n",
        "# Prepare Labels\n",
        "y = df['Sentiment']\n",
        "\n",
        "# Split Dataset\n",
        "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
        "X_train_tfidf, X_test_tfidf, _, _ = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model Training - Decision Tree (Bag of Words)\n",
        "print(\"\\n### Decision Tree with Bag of Words ###\")\n",
        "dt_bow = #TODO\n",
        "dt_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Plot the Decision Tree\n",
        "plt.figure(figsize=(12, 8))\n",
        "plot_tree(dt_bow, max_depth=3, fontsize=10, feature_names=vectorizer_bow.get_feature_names_out(), class_names=dt_bow.classes_, filled=True)\n",
        "plt.title('Decision Tree (Bag of Words)')\n",
        "plt.show()\n",
        "\n",
        "# Evaluation - Bag of Words\n",
        "y_pred_bow = dt_bow.predict(X_test_bow)\n",
        "print(\"Classification Report (Bag of Words):\\n\", classification_report(y_test, y_pred_bow))\n",
        "\n",
        "# Model Training - Decision Tree (TF-IDF)\n",
        "print(\"\\n### Decision Tree with TF-IDF ###\")\n",
        "dt_tfidf = #TODO\n",
        "dt_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Plot the Decision Tree\n",
        "plt.figure(figsize=(12, 8))\n",
        "plot_tree(dt_tfidf, max_depth=3, fontsize=10, feature_names=vectorizer_tfidf.get_feature_names_out(), class_names=dt_tfidf.classes_, filled=True)\n",
        "plt.title('Decision Tree (TF-IDF)')\n",
        "plt.show()\n",
        "\n",
        "# Evaluation - TF-IDF\n",
        "y_pred_tfidf = dt_tfidf.predict(X_test_tfidf)\n",
        "print(\"Classification Report (TF-IDF):\\n\", classification_report(y_test, y_pred_tfidf))\n",
        "\n",
        "# Summary and Comparison\n",
        "print(\"\\n### Summary ###\")\n",
        "print(\"Compare the performance of Bag of Words and TF-IDF features using the metrics.\")\n"
      ],
      "metadata": {
        "id": "CYPyLade50NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion:\n",
        "1. What insights can you draw from the EDA?\n",
        "2. Which feature extraction method performs better? Why?\n",
        "3. Explore hyperparameter tuning for the Decision Tree model.\n",
        "4. Implement semantic feature engineering using pre-trained embeddings (bonus)."
      ],
      "metadata": {
        "id": "T8sZ0kJU6U2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter search example\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Feature Engineering - TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer(max_features=800)\n",
        "X_train_tfidf = vectorizer_tfidf.fit_transform(df['Sentence']).toarray()\n",
        "\n",
        "# Model Training - Decision Tree (Bag of Words)\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Model Training - Decision Tree (TF-IDF)\n",
        "print(\"\\n### Decision Tree with TF-IDF ###\")\n",
        "dt_tfidf = #TODO:\n",
        "\n",
        "# Perform GridSearchCV for hyperparameter tuning\n",
        "grid_search_tfidf = GridSearchCV(dt_tfidf, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "#TODO\n",
        "# Get the best estimator from the grid search\n",
        "dt_tfidf_best = grid_search_tfidf.best_estimator_\n",
        "print(\"Best hyperparameters (TF-IDF):\", grid_search_tfidf.best_params_)"
      ],
      "metadata": {
        "id": "frC4gWKB8ffh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "swT82LuL8lKd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}