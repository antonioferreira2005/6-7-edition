{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIrPAP6rv2UV"
      },
      "source": [
        "# Exercise Notebook: Exploring Text Embeddings and Information Retrieval\n",
        "\n",
        "## Objective:\n",
        "In this exercise, we will:\n",
        "1. Review the concept of text embeddings.\n",
        "2. Implement three information retrieval techniques: TF-IDF, BM25, and Sentence Transformers.\n",
        "3. Visualize the results of these models in 2D and compare their performance for movie recommendation based on title and overview."
      ],
      "id": "uIrPAP6rv2UV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH7ZI_oEv2UW"
      },
      "source": [
        "### 1. Review of Text Embeddings\n",
        "\n",
        "#### What are Embeddings?\n",
        "Embeddings are dense vector representations of text that capture semantic information. Rather than representing words or documents as **sparse one-hot vectors**, embeddings map them to continuous vector spaces where semantically similar items are closer together.\n",
        "\n",
        "There are different techniques to generate embeddings:\n",
        "- **TF-IDF**: Statistical method that reflects how important a word is to a document in a collection.\n",
        "- **BM25**: A probabilistic model that improves upon TF-IDF by considering term frequency saturation and document length.\n",
        "- **Sentence Transformers**: Use deep learning models to generate embeddings for sentences or documents that reflect semantic meaning.\n",
        "\n",
        "\n",
        "Relevant: [one-hot encoding](https://developers.google.com/machine-learning/crash-course/categorical-data/one-hot-encoding)"
      ],
      "id": "JH7ZI_oEv2UW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64f59g7fv2UX"
      },
      "source": [
        "### 2. Dataset\n",
        "We will use the [Movies Dataset from Kaggle](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset) to create a simple movie recommendation system based on titles and overviews.\n",
        "\n",
        "The dataset includes:\n",
        "- **title**: Name of the movie\n",
        "- **overview**: A brief description of the movie\n",
        "\n",
        "#### Task 1: Load the dataset\n",
        "Download the dataset and load it into a Pandas DataFrame.\n"
      ],
      "id": "64f59g7fv2UX"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('movies_metadata.csv')\n",
        "\n",
        "# Keep only the 'title' and 'overview' columns\n",
        "df = df[['title', 'overview']]\n",
        "\n",
        "# Display first few rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3jtV1GLzyvFT"
      },
      "id": "3jtV1GLzyvFT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.1 Explore data\n",
        "Check for missing data, title and overview length distribution and perform a word cloud visualization"
      ],
      "metadata": {
        "id": "oxuA6PBCyzFh"
      },
      "id": "oxuA6PBCyzFh"
    },
    {
      "cell_type": "code",
      "source": [
        "def missing_statistics(df):\n",
        "  #TODO\n",
        "\n",
        "missing_statistics(df)"
      ],
      "metadata": {
        "id": "UWRFIQGrzK9G"
      },
      "id": "UWRFIQGrzK9G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_title_distribution(df):\n",
        "  #TODO\n",
        "\n",
        "compute_title_distribution(df)"
      ],
      "metadata": {
        "id": "IIwgRKB5zPVj"
      },
      "id": "IIwgRKB5zPVj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_overview_distribution(df):\n",
        "  #TODO\n",
        "\n",
        "compute_overview_distribution(df)"
      ],
      "metadata": {
        "id": "IwvKrf07zVQn"
      },
      "id": "IwvKrf07zVQn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def world_cloud(df):\n",
        "  #TODO\n",
        "\n",
        "world_cloud(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "Iy9b550NzYXB",
        "outputId": "a2860a9a-6147-4f3c-8c2f-0c025efd3270"
      },
      "id": "Iy9b550NzYXB",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-1-a29bb360d401>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-a29bb360d401>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def world_cloud(df):\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zETFcalnv2UX"
      },
      "source": [
        "### 3. Data Preprocessing\n",
        "Before we move on to the information retrieval methods, we'll preprocess the data by cleaning the text and performing tokenization. This will help ensure better results from our algorithms.\n",
        "\n",
        "#### Task 2: Clean the text\n",
        "Write a function to clean the text by removing punctuation, converting it to lowercase, and removing digits."
      ],
      "id": "zETFcalnv2UX"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    #TODO\n",
        "    return text"
      ],
      "metadata": {
        "id": "M0DlsnCEwh78"
      },
      "id": "M0DlsnCEwh78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply cleaning function to 'overview' and 'title' columns\n",
        "df['overview'] = df['overview'].apply(clean_text)\n",
        "df['title'] = df['title'].apply(clean_text)\n",
        "\n",
        "# Split data into train and test for evaluation\n",
        "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "#In this notebook we will not be doing evaluation yet, feel free to use all data.\n",
        "\n",
        "# Display a cleaned sample\n",
        "df.head()"
      ],
      "metadata": {
        "id": "dNHg7iR2xJbv"
      },
      "id": "dNHg7iR2xJbv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhhuTN1gv2UX"
      },
      "source": [
        "### 4. TF-IDF Approach\n",
        "#### Task 3: Apply TF-IDF Vectorization\n",
        "Use `TfidfVectorizer` to transform the 'overview' text data into a TF-IDF matrix. Compute the cosine similarity between the movie overviews.\n",
        "\n",
        "Relevant: [TfidfVectorizer](https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n"
      ],
      "id": "vhhuTN1gv2UX"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "-8vgGt5JxS05"
      },
      "id": "-8vgGt5JxS05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and transform the overview data\n",
        "tfidf_vectorizer = #TODO\n",
        "tfidf_matrix = #TODO"
      ],
      "metadata": {
        "id": "KNI-yTbFxWmO"
      },
      "id": "KNI-yTbFxWmO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the cosine similarity matrix\n",
        "cos_sim_tfidf = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Function to get top 5 similar movies based on cosine similarity\n",
        "def get_similar_movies(index, cos_sim_matrix, top_n=5):\n",
        "    similar_indices = cos_sim_matrix[index].argsort()[-top_n-1:-1][::-1]\n",
        "    return train_data.iloc[similar_indices][['title', 'overview']]\n",
        "\n",
        "# Example: Get similar movies for the first movie\n",
        "get_similar_movies(0, cos_sim_tfidf)"
      ],
      "metadata": {
        "id": "g4i5v3f7xOoz"
      },
      "id": "g4i5v3f7xOoz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZFt0fOtv2UX"
      },
      "source": [
        "### 5. BM25 Approach\n",
        "#### Task 4: Apply BM25 (bonus)\n",
        "Use the `rank_bm25` library to compute BM25 scores for the overviews of the movies.\n",
        "\n",
        "```python\n",
        "from rank_bm25 import BM25Okapi\n",
        "```\n",
        "\n",
        "Relevant: [rank_bm25](https://pypi.org/project/rank-bm25/)"
      ],
      "id": "NZFt0fOtv2UX"
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO"
      ],
      "metadata": {
        "id": "Tx4_C-4x3XZ2"
      },
      "id": "Tx4_C-4x3XZ2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whDoWQMFv2UX"
      },
      "source": [
        "### 6. Sentence Transformers Approach\n",
        "#### Task 5: Apply Sentence Transformers\n",
        "Use the `sentence-transformers` library to generate dense embeddings for the titles and overviews. Then compute cosine similarity between the embeddings.\n",
        "\n",
        "Relevant: [sentence-transformers](https://sbert.net/)\n"
      ],
      "id": "whDoWQMFv2UX"
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Initialize pre-trained model\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for titles and overviews\n",
        "embeddings = model.encode#TODO"
      ],
      "metadata": {
        "id": "M2MuLMJ5yQrj"
      },
      "id": "M2MuLMJ5yQrj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_similarity(embeddings, index):\n",
        "    #TODO\n",
        "    raise\n",
        "\n",
        "# Get top 5 similar movies based on sentence embeddings\n",
        "cos_sim_sentence = compute_cosine_similarity(embeddings, 0)\n",
        "top_sentence_indices = cos_sim_sentence.argsort()[-6:-1][::-1]\n",
        "train_data.iloc[top_sentence_indices][['title', 'overview']]"
      ],
      "metadata": {
        "id": "63VmhD7CyVsl"
      },
      "id": "63VmhD7CyVsl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ur134ycv2UY"
      },
      "source": [
        "### 7. Visualization of Embeddings in 2D\n",
        "We will now visualize the embeddings of the three methods (TF-IDF, BM25, and Sentence Transformers) in 2D space using **t-SNE**. This will help us understand which method captures the best semantic relationships.\n",
        "\n",
        "#### Task 6: Visualize the embeddings\n",
        "Use t-SNE to reduce the dimensionality of the embeddings and plot them in 2D.\n",
        "\n",
        "Relevant: [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n",
        "[PCA](https://scikit-learn.org/dev/modules/generated/sklearn.decomposition.PCA.html)"
      ],
      "id": "_Ur134ycv2UY"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Reduce the dimensionality of the embeddings for visualization\n",
        "def plot_embeddings(embeddings, title):\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c='blue', edgecolors='k', alpha=0.7)\n",
        "    plt.title(f'2D Visualization of {title} Embeddings')\n",
        "    plt.xlabel('Dimension 1')\n",
        "    plt.ylabel('Dimension 2')\n",
        "    plt.show()\n",
        "\n",
        "# TF-IDF Visualization\n",
        "plot_embeddings(tfidf_matrix.toarray(), 'TF-IDF')\n",
        "\n",
        "# Sentence Transformer Visualization\n",
        "plot_embeddings(embeddings, 'Sentence Transformers')"
      ],
      "metadata": {
        "id": "oprfVqi0ynPS"
      },
      "id": "oprfVqi0ynPS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 6.1\n",
        "You showed the plots you just did above to your boss but he is finding it hard to understand if the embeddings are capturing any meaningfull value.\n",
        "What do you do?"
      ],
      "metadata": {
        "id": "3p2CN1CF4wlB"
      },
      "id": "3p2CN1CF4wlB"
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO\n",
        "#maybe look at the rest of the data and try to put better labels on the plot so its easier to understand ?"
      ],
      "metadata": {
        "id": "7gYkgkpq4vCd"
      },
      "id": "7gYkgkpq4vCd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDeXX4_qv2UY"
      },
      "source": [
        "### 8. Discussion and Comparison\n",
        "#### Questions for the students to reflect on:\n",
        "- **Why do we clean the data?**\n",
        "- **Do we always need to clean the data?**\n",
        "- **What is the conceptual difference between syntatic and semantic embeddings?**\n",
        "- **Which embedding method appears to capture the best semantic relationships between movies?**\n",
        "- **Do the 2D visualizations show any clear clusters or patterns?**\n",
        "- **How do the different algorithms perform when compared in terms of recommendation accuracy?**\n",
        "- **Can the additional metadata (e.g., genres, actors) improve the system? If so, how?**"
      ],
      "id": "VDeXX4_qv2UY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVHhPZdcv2UY"
      },
      "source": [
        "### 9. Extension Ideas\n",
        "- **Improve with additional metadata**: You can integrate other columns like 'genres', 'cast', or 'director' to generate richer embeddings.\n",
        "- **Evaluation**: Perform precision and recall evaluation on your retrieval methods by comparing the top recommendations against a manually curated set of similar movies - we will be doing this in a future exercise today.\n",
        "- **Model Fine-Tuning**: Fine-tune a Sentence-Transformer model on your dataset to improve the embeddings."
      ],
      "id": "HVHhPZdcv2UY"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}