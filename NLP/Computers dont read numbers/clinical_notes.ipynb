{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treat Clinical notes\n",
    "\n",
    "**COLAB link** https://colab.research.google.com/github/samsung-ai-course/6-7-edition/blob/main/NLP/Computers%20dont%20read%20numbers/clinical_notes.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import re\n",
    "import requests \n",
    "from tqdm import tqdm \n",
    "\n",
    "# Data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data files (run only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files into a Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = requests.get(\"https://raw.githubusercontent.com/samsung-ai-course/6-7-edition/refs/heads/main/NLP/Computers%20dont%20read%20numbers/directories.txt\")\n",
    "files_list  = files_list.text.split(\"\\n\")\n",
    "notes = []\n",
    "# run this cells only once this can be blocked by github if a lot of runs are tried\n",
    "for file in tqdm(files_list[:15]):\n",
    "\n",
    "    note = requests.get(\"https://raw.githubusercontent.com/samsung-ai-course/6-7-edition/refs/heads/main/NLP/Computers%20dont%20read%20numbers/data/\"+file)\n",
    "    notes.append(note.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here everything is done dont worry\n",
    "# replacement of None Values Exercise\n",
    "def replace_placeholders(text, placeholder=\"___\", replacement=\"None\"):\n",
    "    return text.replace(placeholder, replacement)\n",
    "\n",
    "# Extracting key information using regex\n",
    "def extract_information(text):\n",
    "    info = {}\n",
    "    patterns = {\n",
    "        \"Name\": r\"Name:\\s+(.*?)\\s+Unit No\",\n",
    "        \"Unit No\": r\"Unit No:\\s+(.*?)\\n\",\n",
    "        \"Admission Date\": r\"Admission Date:\\s+(.*?)\\s+Discharge Date\",\n",
    "        \"Discharge Date\": r\"Discharge Date:\\s+(.*?)\\n\",\n",
    "        \"Date of Birth\": r\"Date of Birth:\\s+(.*?)\\s+Sex\",\n",
    "        \"Sex\": r\"Sex:\\s+(\\w)\",\n",
    "        \"Service\": r\"Service:\\s+(.*?)\\n\",\n",
    "        \"Allergies\": r\"Allergies:\\s+(.*?)\\n\",\n",
    "        \"Chief Complaint\": r\"Chief Complaint:\\n(.*?)\\n\",\n",
    "        \"Major Surgical or Invasive Procedure\": r\"Major Surgical or Invasive Procedure:\\n(.*?)\\n\",\n",
    "        \"History of Present Illness\": r\"History of Present Illness:\\n(.*?)\\n\\n\",\n",
    "        \"Review of systems\": r\"Review of systems:\\s+(.*?)\\n\\n\",\n",
    "        \"Past Medical History\": r\"Past Medical History:\\n(.*?)\\n\\n\",\n",
    "        \"Social History\": r\"Social History:\\n(.*?)\\n\",\n",
    "        \"Family History\": r\"Family History:\\n(.*?)\\n\\n\",\n",
    "        \"Physical Exam\": r\"Physical Exam:\\n(.*?)\\n\\n\",\n",
    "        \"Brief Hospital Course\": r\"Brief Hospital Course:\\n(.*?)\\n\\n\",\n",
    "        \"Discharge Diagnosis\": r\"Discharge Diagnosis:\\n(.*?)\\n\\n\",\n",
    "        \"Discharge Condition\": r\"Discharge Condition:\\n(.*?)\\n\\n\",\n",
    "        \"Discharge Instructions\": r\"Discharge Instructions:\\n(.*?)\\n\\n\",\n",
    "    }\n",
    "\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        info[key] = match.group(1).strip() if match else \"None\"\n",
    "        \n",
    "    return info\n",
    "def structure_data(clinical_text):\n",
    "    # Replace placeholders in the clinical text\n",
    "    processed_text = replace_placeholders(clinical_text)\n",
    "\n",
    "    # Extract information into a dictionary\n",
    "    return extract_information(processed_text)\n",
    "\n",
    "structure_notes = [ structure_data(note) for note in notes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null values treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(structure_notes).replace('None',np.NaN)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with nan values\n",
    "df_treated = df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start building a corpus\n",
    "discharged_diagnosis = ' '.join(df_treated['Discharge Diagnosis'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to clean individual lines\n",
    "def clean_line(line):\n",
    "    # Remove HTML-like tags\n",
    "    line = NotImplementedError\n",
    "    # Remove unwanted characters (e.g., non-ASCII characters)\n",
    "    line = re.sub(r'[^\\x00-\\x7F]+', '', line)\n",
    "    # Strip leading/trailing whitespace\n",
    "    return line.strip()\n",
    "\n",
    "def speaking_text_cleaner(speaking_text):\n",
    "    # Process each line to clean it\n",
    "    cleaned_lines = []\n",
    "    for line in speaking_text.splitlines():\n",
    "        cleaned_line = clean_line(line)\n",
    "        if cleaned_line:  # Only add non-empty lines\n",
    "            cleaned_lines.append(cleaned_line)\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "cleaned_discharged_diagnosis = speaking_text_cleaner(discharged_diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_cleaner(cleaned_text,n_min = 4 ):\n",
    "    n_min = 4                                                           # Minimum number of characters. \n",
    "    corpus = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    pre = re.sub(r'\\W', ' ', cleaned_text)                          # Substitute the non-alphanumerics character by space. \n",
    "                                        # Remove numbers.                                      # Exercise\n",
    "    pre = nltk.word_tokenize(pre)                                   # Tokenize into words.                                 # Exercise\n",
    "    pre = [x for x in pre if len(x) > n_min]                        # Minimum length.\n",
    "    pre = [x.lower() for x in pre]                                  # Convert into the lowercase.\n",
    "    pre = [x for x in pre if x not in stopwords.words('english')]   # Remove stopwords.\n",
    "    pre = [lemmatizer.lemmatize(x) for x in pre]                    # Lemmatize.\n",
    "    corpus += pre                                                   # Back to the corpus.\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_clean  = corpus_cleaner( cleaned_discharged_diagnosis )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# choose words you want to remove\n",
    "remove_words = ['primary','secondary','going','diagnosis','right']\n",
    "  = [x for x in corpus_clean if x not in remove_words]\n",
    "pd.Series(Counter(corpus_clean)).sort_values()[-50:].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_long_sentence = ' '.join(treated_corpus)\n",
    "wc = WordCloud(background_color='white', max_words=70,colormap='gist_gray')                  # Customize the output.\n",
    "wc.generate(a_long_sentence)\n",
    "# wc.words_                                                          # Check for the top ranking words.                                                         \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")                                    # Turn off the axes.\n",
    "#plt.savefig('clinical_notes.png',bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
