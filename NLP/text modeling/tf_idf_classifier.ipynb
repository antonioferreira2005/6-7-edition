{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/bullshit.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/dr_robot_house.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, you‚Äôre stepping into the shoes of a fake news detector. Your mission? Detect fake news like a pro and become the hero the world didn‚Äôt know it needed (no cape required).  \n",
    "\n",
    "Here‚Äôs the deal:  \n",
    "We‚Äôve got a dataset loaded with news articles. Some are real, and some are absolute baloney üêÇüí©. Your job is to **clean, standardize, and model the data** to predict whether an article is fake or real. Think of it as training an AI-powered radar to sniff out misinformation. Cool, right?  \n",
    "\n",
    "### **About the Dataset**  \n",
    "This isn‚Äôt just random internet junk‚Äîwe‚Äôre working with articles collected from real-world sources:  \n",
    "- **Real News**: Crawled from *Reuters.com*, a trusted news source.  \n",
    "- **Fake News**: Gathered from unreliable sites flagged by **Politifact** and **Wikipedia**.  \n",
    "\n",
    "The dataset covers a range of topics, but it leans heavily toward political and world news‚Äîareas where fake news thrives.  \n",
    "\n",
    "### **Dataset Columns**  \n",
    "Here‚Äôs a breakdown of the key information you‚Äôll be working with:  \n",
    "- **title**: The headline of the article.  \n",
    "- **text**: The full content of the article.  \n",
    "- **subject**: The category or topic of the article.  \n",
    "- **date**: When it was published.  \n",
    "- **Label**: Whether it‚Äôs real (1) or fake (0).  \n",
    "\n",
    "### **How This Works**  \n",
    "You‚Äôll be:  \n",
    "1. **Cleaning up messy text** by removing junk.\n",
    "2. **Standardizing** the data to make it machine-readable, remove stop words and reducing words to their roots (because \"running\", \"ran\", and \"runs\" are basically the same thing).  \n",
    "3. **Training a model** to become your official Bullsh*t Detector.  \n",
    "\n",
    "By the end of this notebook, you‚Äôll have the skills to spot fake news faster than a fact-checker and impress anyone who‚Äôs tired of misinformation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text processing and NLP\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Model building and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trump‚Äôs Own Staff Thinks He‚Äôs A Joke, Laughs ...</td>\n",
       "      <td>As the Russian investigation is getting uncomf...</td>\n",
       "      <td>News</td>\n",
       "      <td>June 12, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kellyanne Conway Just Earned ‚ÄòLeaker‚Äô Label, ...</td>\n",
       "      <td>The entire Trump White House is all about crac...</td>\n",
       "      <td>News</td>\n",
       "      <td>June 10, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Secret Service Officially Can‚Äôt Afford To Pay...</td>\n",
       "      <td>Donald Trump has been wasting taxpayer dollars...</td>\n",
       "      <td>News</td>\n",
       "      <td>August 21, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sicko In Texas Wants To Make A Buck Off Harve...</td>\n",
       "      <td>Late Monday night, CNN s Don Lemon had a phone...</td>\n",
       "      <td>News</td>\n",
       "      <td>August 29, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>France‚Äôs President Threw AWESOME Shade At Tru...</td>\n",
       "      <td>French President Emmanuel Macron wasted no tim...</td>\n",
       "      <td>News</td>\n",
       "      <td>June 2, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Trump‚Äôs Own Staff Thinks He‚Äôs A Joke, Laughs ...   \n",
       "1   Kellyanne Conway Just Earned ‚ÄòLeaker‚Äô Label, ...   \n",
       "2   Secret Service Officially Can‚Äôt Afford To Pay...   \n",
       "3   Sicko In Texas Wants To Make A Buck Off Harve...   \n",
       "4   France‚Äôs President Threw AWESOME Shade At Tru...   \n",
       "\n",
       "                                                text subject             date  \\\n",
       "0  As the Russian investigation is getting uncomf...    News    June 12, 2017   \n",
       "1  The entire Trump White House is all about crac...    News    June 10, 2017   \n",
       "2  Donald Trump has been wasting taxpayer dollars...    News  August 21, 2017   \n",
       "3  Late Monday night, CNN s Don Lemon had a phone...    News  August 29, 2017   \n",
       "4  French President Emmanuel Macron wasted no tim...    News     June 2, 2017   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/news.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Cleaning function\n",
    "\n",
    "Ok the usual lets clean the data\n",
    "\n",
    "![](media/clean.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    text = re.sub('[.*?]|\\w*\\d\\w*|\\n|https?:\\\\\\S+|www\\.\\S+|<.*?>+', ' ', text)\n",
    "    #text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\\\W', \" \", text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Standardize data\n",
    "In this section, you are tasked with completing the function standardize_data. Your goal is to process a given text by:\n",
    "\n",
    " 1. Converting everything to lowercase: Ensures uniformity. \n",
    "    \n",
    " 2. Removing stop words: Filters out common words like \"the\", \"and\", or \"is\" that don't add much meaning. \n",
    "    \n",
    " 3. Applying either stemming or lemmatization: Both are used to reduce words to their base form, but note:  \n",
    "    - **Stemming** is faster and simpler but may produce non-dictionary roots (e.g., \"studies\" ‚Üí \"studi\").\n",
    "    \n",
    "    - **Lemmatization** is more accurate but requires more computational power (e.g., \"running\" ‚Üí \"run\").\n",
    "\n",
    " 4. Create a new column by applying this to either the title or the text or if you prefer you can concatenate both. call this new column `std_text` \n",
    "Below is an example of how you can use stemming, lemmatization, and stop words to process text:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Below is an example of how you can use stemming, lemmatization, and stop words to process text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'those', 'your', 'y', 'ma', \"she's\", \"don't\", \"haven't\", 'weren', \"won't\", 'yourselves', 'o', 'yourself', 'needn', 'for', 'doing', 'himself', 'because', 'more', 'at', 'where', 'yours', 'off', 'theirs', \"doesn't\", 'below', 'shan', \"isn't\", 'than', 'which', 'itself', 'between', 'mustn', 'all', 'am', \"hasn't\", 'a', 'by', 'out', 'while', 'up', 'can', 'he', \"mightn't\", \"mustn't\", 'hadn', 'just', 'our', 'were', 'over', 'too', 'until', 'down', 'other', \"shouldn't\", 'how', 'again', 'm', 'and', 'most', 'then', 'against', 'from', 'only', 'very', \"you're\", 'hers', 'themselves', 'd', 'don', 'further', 'same', 'did', 'about', 'some', 'these', 'of', \"shan't\", 'as', 'now', 'her', 'they', 'wouldn', 'here', \"you'd\", 'in', 'with', 'was', 're', 'them', 'being', 'above', 'its', 's', 'there', 't', 'is', 'the', 'nor', 'this', 'both', 'ain', 'aren', 'that', 'why', 'once', \"hadn't\", \"wasn't\", 'on', 'their', 'any', \"didn't\", 'own', 'to', 'after', \"needn't\", 'having', 'not', 'an', 'myself', 'him', 'are', 'into', 'through', 'ourselves', 'wasn', \"you've\", \"it's\", 'll', 'do', \"weren't\", \"couldn't\", 'mightn', 'haven', \"wouldn't\", 'you', 'isn', 'does', 'his', 'couldn', 'if', 'few', 'she', 'be', 'ours', 'each', 'has', 'will', 'but', 'no', 'who', 've', 'what', 'have', \"you'll\", 'me', 'during', 'had', 'shouldn', 'under', 'been', 'before', 'doesn', 'should', 'so', 'i', 'herself', \"aren't\", 'such', 'when', 'didn', 'my', 'hasn', 'it', 'or', \"should've\", \"that'll\", 'whom', 'won', 'we'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: running\n",
      "Stemmed: run\n",
      "Lemmatized: run\n"
     ]
    }
   ],
   "source": [
    "# How does it work\n",
    "\n",
    "# Initialize SnowballStemmer and WordNetLemmatizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example word\n",
    "word = \"running\"\n",
    "\n",
    "# Apply both stemming and lemmatization\n",
    "stemmed = stemmer.stem(word)\n",
    "lemmatized = lemmatizer.lemmatize(word, pos=\"v\")  # 'v' for verb\n",
    "\n",
    "print(f\"Word: {word}\")\n",
    "print(f\"Stemmed: {stemmed}\")\n",
    "print(f\"Lemmatized: {lemmatized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['Studies', 'show', 'that', 'the', 'happiest', 'people', 'normally', 'like', 'running']\n",
      "Stemmed words: ['studi', 'show', 'that', 'the', 'happiest', 'peopl', 'normal', 'like', 'run']\n",
      "Lemmatized words: ['study', 'show', 'that', 'the', 'happiest', 'people', 'normally', 'like', 'run']\n"
     ]
    }
   ],
   "source": [
    "# Sentence to analyze\n",
    "sentence = \"Studies show that the happiest people normally like running\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "\n",
    "# Apply both stemming and lemmatization\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word.lower(), pos=\"v\") for word in words]\n",
    "\n",
    "# Display the results\n",
    "print(\"Original words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)\n",
    "print(\"Lemmatized words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Task\n",
    " - Complete the function standardize_data to process a list of sentences.\n",
    " - Decide whether to use stemming or lemmatization based on your preference or system performance constraints.\n",
    " - Use the provided example as a reference for handling tokenization, stop words, and standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint\n",
    "Remember:\n",
    "- Use `word_tokenize` for splitting sentences into tokens.  \n",
    "  \n",
    "- Use the `stopwords.words(\"english\")` from `nltk` to filter out stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def standardize_data(sentence,stopwords):\n",
    "    #convert lower case\n",
    "    #tokenize\n",
    "    #remove stop words\n",
    "    #standardize words using stemming or lematization\n",
    "\n",
    "    # join all the treated tokens again \n",
    "    return ' '.join(treated_tokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Word Clouds\n",
    "\n",
    "to create a word cloud concatenate all the texts into a very long string and then let the `WordCloud` deal with the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ' '.join(df['std_text'])\n",
    "wc = WordCloud(background_color='white', max_words=70)#,colormap='gist_gray')                  # Customize the output.\n",
    "wc.generate(a_long_sentence)\n",
    "# wc.words_                                                          # Check for the top ranking words.                                                         \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")                                    # Turn off the axes.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the code above create one word cloud for Label 0 and label 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Create more features ( Optional )\n",
    "This is an optional part if you have time you can added to the documents and analysis simple features like lenght of the title, text, average length of each word..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vectorize using tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tf-idf vectorizer compare the amount of generate variables with `std_text` and the original variable `text`. To do this use `len(tfidf.get_feature_names_out())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=100)\n",
    "tfidf.fit()\n",
    "len(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Vectorize and Model it \n",
    "![](media/predict.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X\n",
    "\n",
    "# Create the TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features = 10)\n",
    "X = tfidf.fit_transform(df['text'])\n",
    "X_ = X.toarray()  \n",
    "y = df['label']  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the first try ignore this cell Now if you want you can convert this into a dataframe and add more variables. \n",
    "\n",
    "## Get the feature names (words) from the TfidfVectorizer\n",
    "# words = tfidf.get_feature_names_out()\n",
    "## Convert the TF-IDF matrix (X_) to a DataFrame\n",
    "# X_df = pd.DataFrame(X_, columns=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training dataset and create the model\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model using .fit method\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions using .predict\n",
    "y_pred= model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model\n",
    "![](media/eval.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation Calculating metrics \n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"accuracy is {np.round(accuracy*100,3)} %\")\n",
    "\n",
    "\n",
    "#  Calculate and display the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "disp.plot(cmap='Blues')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
